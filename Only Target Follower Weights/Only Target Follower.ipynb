{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "from decimal import Decimal\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bicycle and its environment\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "class CycleBalancingEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}  \n",
    "  \n",
    "    def __init__(self):\n",
    "        # Out cycle has only 1 action spaces i.e. The position of the handlebar\n",
    "        self.action_space = gym.spaces.box.Box(\n",
    "            low=-1 * np.ones(1, dtype=np.float32),\n",
    "            high=1 * np.ones(1, dtype=np.float32))\n",
    "        # Obervation space\n",
    "        self.observation_space = gym.spaces.box.Box(\n",
    "            low=-1 * np.ones(6, dtype=np.float32),\n",
    "            high=1 * np.ones(6, dtype=np.float32))\n",
    "        self.np_random, _ = gym.utils.seeding.np_random()\n",
    "\n",
    "        if not p.isConnected():\n",
    "            self.client = p.connect(p.GUI)\n",
    "        else:\n",
    "            self.client = 1\n",
    "        #self.client = p.connect(p.SHARED_MEMORY)        \n",
    "        #self.client = p.connect(p.DIRECT)\n",
    "        \n",
    "        self.n_target = 0\n",
    "        self.min_target_dist = 10\n",
    "        self.target_span = 100\n",
    "        self.sphere_dist = 1.5\n",
    "        self.pole = []\n",
    "        p.resetSimulation(self.client)\n",
    "        p.setRealTimeSimulation(0)\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        self.plane=p.loadURDF(\"plane.urdf\",[0,0,0], useFixedBase=True)\n",
    "        self.bike = 0\n",
    "        self.angle_span = 20\n",
    "        self.n_episodes = 0\n",
    "        self.rays_distance = 30\n",
    "        self.z_balance = -0.25\n",
    "        self.z_target = -1\n",
    "        \n",
    "        self.make_obstacles()\n",
    "        self.reset()\n",
    "        #self.show_img()\n",
    "        \n",
    "    # Fuction to show the Distance traveled by rays in an image.\n",
    "    def show_img(self):\n",
    "        self.img = np.zeros((800,800,3), dtype='float32')\n",
    "        shift = 400\n",
    "        multiply = 400\n",
    "        ls = p.getBasePositionAndOrientation(self.bike)\n",
    "        bike_x = ls[0][0]\n",
    "        bike_y = ls[0][1]\n",
    "        handlebar_rotation = p.getEulerFromQuaternion( p.getLinkState(self.bike, 0)[1] )[2]\n",
    "        mini = 1000\n",
    "        for deg in range(1, 361, 1):\n",
    "            mini = min(mini, self.dist[deg-1])\n",
    "            if deg%self.angle_span==0:\n",
    "                rad = Decimal( Decimal(deg * np.pi/180 + handlebar_rotation)%Decimal(2*np.pi) + Decimal(2*np.pi))%Decimal(2*np.pi)\n",
    "                rad = float(rad)\n",
    "                start = (int(shift + bike_x + self.sphere_dist*np.cos(rad)), int(shift + bike_y + self.sphere_dist*np.sin(rad)))\n",
    "                end = (int(shift + bike_x + mini*multiply*np.cos(rad)), int(shift + bike_y + mini*multiply*np.sin(rad)))\n",
    "                cv2.ellipse(self.img, start, (int(mini*multiply),int(mini*multiply)), 0, (rad*180/np.pi)-self.angle_span, (rad*180/np.pi), (0,0,255), -1)\n",
    "                mini = 1000\n",
    "        cv2.imshow('img', cv2.rotate(cv2.transpose(self.img), cv2.ROTATE_180))\n",
    "        cv2.waitKey(1)\n",
    "        \n",
    "    # Step Function which take action as input and performs that action and returns the reward for that action as well as the next observation state\n",
    "    def step(self, action):\n",
    "        p.setJointMotorControl2(self.bike, 0, p.POSITION_CONTROL, targetPosition=action[0], maxVelocity=5) # Apply Position control to Handlebar\n",
    "        for i in range(3):\n",
    "            p.setJointMotorControl2(self.bike,1,p.TORQUE_CONTROL , force=(2.5+0)*10000) # Apply Toruqe to Back Wheel\n",
    "            p.setJointMotorControl2(self.bike,2,p.TORQUE_CONTROL , force=(2.5+0)*10000) # Apply Toruqe to Front Wheel\n",
    "            ls = p.getBasePositionAndOrientation(self.bike) # ls[0]=Postion of cycle, ls[1] = Orientation of cycle\n",
    "            val = p.getEulerFromQuaternion(ls[1])[0] - 1.57 # Calculating inclination of cycle from vertical\n",
    "            p.applyExternalTorque(self.bike, -1, [-1000000*val, 0, 0], flags=p.WORLD_FRAME)\n",
    "            p.stepSimulation()\n",
    "        \n",
    "        ls = p.getBasePositionAndOrientation(self.bike) # ls[0]=Postion of cycle, ls[1] = Orientation of cycle\n",
    "        val = p.getEulerFromQuaternion(ls[1])[0] - 1.57 # Calculating inclination of cycle from vertical\n",
    "        z = ls[0][2] + self.z_balance\n",
    "            \n",
    "        tmp = ls[0]\n",
    "        self.bike_x = tmp[0]\n",
    "        self.bike_y = tmp[1]\n",
    "        \n",
    "        obs = [] # Observation Space\n",
    "        obs.append(np.arctan( (self.target_x-self.bike_x)/(self.target_y-self.bike_y) ))\n",
    "        ls = p.getBasePositionAndOrientation(self.bike)\n",
    "        obs += p.getEulerFromQuaternion(ls[1])\n",
    "        obs.append((ls[0][0] - self.target_x)/self.target_span)\n",
    "        obs.append((ls[0][1] - self.target_y)/self.target_span)\n",
    "    \n",
    "        bike_x = ls[0][0]\n",
    "        bike_y = ls[0][1]\n",
    "        reward_2 = 0\n",
    "        #cnt = 0\n",
    "        ray_from = []\n",
    "        ray_to = []\n",
    "        handlebar_rotation = p.getEulerFromQuaternion( p.getLinkState(self.bike, 0)[1] )[2]\n",
    "\n",
    "        self.time += 1 # Adding 1 to the time for which the current episode has been running\n",
    "        \n",
    "        # Terminating the episode if the cycle covers less than 1 units distance in 200 timesteps\n",
    "        dist_2 = np.sqrt((self.bike_x)**2 + abs(self.bike_y)**2)\n",
    "        reward_3 = 0\n",
    "        if dist_2 > (np.sqrt(self.target_x**2 + self.target_y**2) + 10): \n",
    "            self.done = True\n",
    "            reward_3 = -500\n",
    "            print(\"Outside Range!\")\n",
    "        if self.time%10==0 and dist_2>self.distance: self.distance = dist_2\n",
    "            \n",
    "        if self.time>999:\n",
    "            reward_3 = -500\n",
    "        \n",
    "        value = p.getEulerFromQuaternion(p.getLinkState(self.bike, 0)[1])[2] - p.getEulerFromQuaternion(ls[1])[2]\n",
    "        if value<-1: value += 2*np.pi\n",
    "        if value>1: value -= 2*np.pi\n",
    "        #print(value)\n",
    "        if value < -0.5:\n",
    "            self.left += 0.1\n",
    "            self.right = 0\n",
    "        elif value > 0.5:\n",
    "            self.left = 0\n",
    "            self.right += 0.1\n",
    "        else:\n",
    "            self.left = 0\n",
    "            self.right = 0\n",
    "            \n",
    "        self.neg_reward = 0\n",
    "        if self.left>10 or self.right>10:\n",
    "            print(\"Slow!!!\")\n",
    "            self.neg_reward = -700\n",
    "            self.done = True\n",
    "                        \n",
    "        val = self.target_span\n",
    "        reward_1 = 0\n",
    "        dist_3 = np.sqrt( (self.bike_x - self.target_x)**2 + (self.bike_y - self.target_y)**2 )\n",
    "        if dist_3 < self.target_distance:\n",
    "            reward_1 = 100 + self.target_reward\n",
    "            self.target_distance -= 5\n",
    "            self.target_reward = min(500, self.target_reward*2)\n",
    "            \n",
    "        self.completed = 0\n",
    "        if dist_3 < 10:\n",
    "            reward_1 = 500\n",
    "            self.completed = 1\n",
    "            self.done = True\n",
    "            print(\"DONE!\")\n",
    "            self.make_obstacles()\n",
    "        \n",
    "        # Calculating the total reward\n",
    "        reward = reward_1 - abs(ls[0][0] - self.target_x)/10. - abs(ls[0][1] - self.target_y)/10. + self.neg_reward - self.left - self.right + reward_3\n",
    "        #print(mini, end=\" \")\n",
    "        \n",
    "        if self.done:\n",
    "            print(self.left, self.right, dist_3, self.target_distance)\n",
    "\n",
    "        obs = np.array(obs, dtype=np.float32)\n",
    "\n",
    "#         if self.time%10==0 and not self.done:\n",
    "#             self.show_img()\n",
    "\n",
    "        return obs, reward/100, self.done, dict()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.n_episodes += 1\n",
    "        if self.n_episodes==20:\n",
    "            self.make_obstacles()\n",
    "            self.n_episodes = 0\n",
    "        \n",
    "        if self.bike!=0:\n",
    "            p.removeBody(self.bike)\n",
    "        # Loading the cycle\n",
    "        self.bike_x = 0 # random.randint(-5, 5) # X position of the cycle\n",
    "        self.bike_y = 0 # random.randint(-5, 5) # Y position of the cycle\n",
    "        self.bike=p.loadURDF(\"bike_2.urdf.xml\",[self.bike_x, self.bike_y,0], p.getQuaternionFromEuler([0,0, random.random()*2*np.pi]),  useFixedBase=False)\n",
    "        \n",
    "        for i in range(10):\n",
    "            p.stepSimulation()\n",
    "            \n",
    "        \n",
    "        # Adding friction and other dynamics\n",
    "        p.changeDynamics(self.plane, -1, lateralFriction=5, angularDamping=1)\n",
    "        p.changeDynamics(self.bike, 1, mass=100)\n",
    "        p.changeDynamics(self.bike, -1, lateralFriction=5, angularDamping=1)\n",
    "        \n",
    "        p.setGravity(0, 0, -250) # Setting the gravity\n",
    "        #p.setRealTimeSimulation(0)\n",
    "        self.done=False\n",
    "        self.time = 0\n",
    "        self.distance = np.sqrt((self.bike_x)**2 + abs(self.bike_y)**2)\n",
    "        self.neg_reward = 0\n",
    "        \n",
    "        obs = []\n",
    "        obs.append( np.arctan( (self.target_x-self.bike_x)/(self.target_y-self.bike_y) ) )\n",
    "        ls = p.getBasePositionAndOrientation(self.bike)\n",
    "        obs += p.getEulerFromQuaternion(ls[1])\n",
    "        obs.append((ls[0][0] - self.target_x)/self.target_span)\n",
    "        obs.append((ls[0][1] - self.target_y)/self.target_span)\n",
    "        \n",
    "        self.cnt = 0\n",
    "        self.left = 0\n",
    "        self.right = 0\n",
    "        self.target_distance = (np.sqrt( (self.bike_x - self.target_x)**2 + (self.bike_y - self.target_y)**2))//10 * 10\n",
    "        self.target_reward = 128\n",
    "        self.completed = 0\n",
    "        \n",
    "        obs = np.array(obs, dtype=np.float32)\n",
    "        return obs\n",
    "\n",
    "    def make_obstacles(self):\n",
    "        \n",
    "        p.resetSimulation(self.client)\n",
    "        p.setRealTimeSimulation(0)\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        #self.plane=p.loadURDF(\"plane.urdf\",[0,0,0], useFixedBase=True)\n",
    "        mul = 100\n",
    "        height = 2\n",
    "        visualShift = [0, 0, 0]\n",
    "        shift = [0, 0, 0]\n",
    "        meshScale=[0.1*mul, 0.1*mul, 0.1*mul*height]\n",
    "        path = 'C:/Users/User/Documents/GitHub/bullet3/examples/pybullet/gym/pybullet_data/'\n",
    "        groundColId = p.createCollisionShape(shapeType=p.GEOM_MESH, \n",
    "                                                  fileName=os.path.join(path, \"terrain.obj\"), \n",
    "                                                  collisionFramePosition=shift,\n",
    "                                                  meshScale=meshScale,\n",
    "                                                  flags=p.GEOM_FORCE_CONCAVE_TRIMESH)\n",
    "        groundVisID = p.createVisualShape(shapeType=p.GEOM_MESH, \n",
    "                                            fileName=os.path.join(path, \"terrain.obj\"), \n",
    "                                            rgbaColor=[0.7,0.3,0.1,1],\n",
    "                                            specularColor=[0.4,.4,0],\n",
    "                                            visualFramePosition=visualShift,\n",
    "                                            meshScale=meshScale)\n",
    "        self.plane = p.createMultiBody(baseMass=0,\n",
    "                                              baseInertialFramePosition=[0,0,0],\n",
    "                                              baseCollisionShapeIndex=groundColId, \n",
    "                                              baseVisualShapeIndex=groundVisID, \n",
    "                                              basePosition=[0,0,0], \n",
    "                                              useMaximalCoordinates=True)\n",
    "        self.bike = 0\n",
    "            \n",
    "        self.bike_x = 0\n",
    "        self.bike_y = 0\n",
    "        self.pole = []\n",
    "        for i in range(self.n_target):\n",
    "            target_x = self.bike_x\n",
    "            target_y = self.bike_y\n",
    "            while (np.sqrt( (self.bike_x - target_x)**2 + (self.bike_y - target_y)**2 )) < self.min_target_dist:\n",
    "                target_x = random.randint(int(self.bike_x) - self.target_span, int(self.bike_x) + self.target_span)\n",
    "                target_y = random.randint(int(self.bike_y) - self.target_span, int(self.bike_y) + self.target_span)\n",
    "            self.pole.append( p.loadURDF(\"C:/Users/User/Documents/GitHub/bullet3/examples/pybullet/gym/pybullet_data/cube.urdf\",[target_x, target_y, 4], [0,0,0,1], useFixedBase=True, globalScaling=1.0) )\n",
    "            p.changeDynamics(self.pole[i], -1, mass=1000)\n",
    "            \n",
    "        # Loading the target\n",
    "        self.pole = []\n",
    "        min_target_range = 90\n",
    "        for i in range(1):\n",
    "            self.target_x = 0\n",
    "            self.target_y = 0\n",
    "            while (np.sqrt( (self.bike_x - self.target_x)**2 + (self.bike_y - self.target_y)**2 )) < 90:\n",
    "                self.target_x = random.randint(int(self.bike_x) - self.target_span, int(self.bike_x) + self.target_span)\n",
    "                self.target_y = random.randint(int(self.bike_y) - self.target_span, int(self.bike_y) + self.target_span)\n",
    "            self.pole.append( p.loadURDF(\"C:/Users/User/Documents/GitHub/bullet3/examples/pybullet/gym/pybullet_data/cube.urdf\",[self.target_x, self.target_y, 2], [0,0,0,1], useFixedBase=True, globalScaling=5.0) )\n",
    "        self.target_distance = (np.sqrt( (self.bike_x - self.target_x)**2 + (self.bike_y - self.target_y)**2))//10 * 10\n",
    "        self.target_reward = 128\n",
    "            \n",
    "    def make_sphere(self):\n",
    "        for i in self.sphere:\n",
    "            p.removeBody(i)\n",
    "            \n",
    "        ls = p.getBasePositionAndOrientation(self.bike)\n",
    "        z = ls[0][2] + self.z_balance\n",
    "        bike_x = ls[0][0]\n",
    "        bike_y = ls[0][1]\n",
    "        self.sphere = []\n",
    "        handlebar_rotation = p.getEulerFromQuaternion( p.getLinkState(self.bike, 0)[1] )[2]\n",
    "        for deg in range(1, 361, 10):\n",
    "            rad = Decimal( Decimal(deg * np.pi/180 + handlebar_rotation)%Decimal(2*np.pi) + Decimal(2*np.pi))%Decimal(2*np.pi)\n",
    "            rad = float(rad)\n",
    "            #self.sphere.append(p.loadURDF('sphere_small.urdf', [bike_x + self.sphere_dist*np.cos(rad), bike_y + self.sphere_dist*np.sin(rad), z], [0,0,0,1]))\n",
    "            #p.loadURDF('sphere_small.urdf', [bike_x + (self.sphere_dist+1*rad)*np.cos(rad), bike_y + (self.sphere_dist+1*rad)*np.sin(rad), 1], [0,0,0,1], useFixedBase=True, globalScaling=deg/10)\n",
    "            self.sphere.append(p.loadURDF('sphere_small.urdf', [bike_x + self.rays_distance*np.cos(rad), bike_y + self.rays_distance*np.sin(rad), z+(abs(deg-180)*(-self.rays_distance/90.)+self.rays_distance)*np.tan(p.getEulerFromQuaternion(ls[1])[1])], [0,0,0,1], useFixedBase=True))\n",
    "        print(ls[1], p.getEulerFromQuaternion(ls[1]))\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "            \n",
    "        #p.stepSimulation()\n",
    "        \n",
    "        distance=5\n",
    "        yaw = 0\n",
    "        humanPos, humanOrn = p.getBasePositionAndOrientation(self.bike)\n",
    "        humanBaseVel = p.getBaseVelocity(self.bike)\n",
    "        #print(\"frame\",frame, \"humanPos=\",humanPos, \"humanVel=\",humanBaseVel)\n",
    "        camInfo = p.getDebugVisualizerCamera()\n",
    "        curTargetPos = camInfo[11]\n",
    "        distance=camInfo[10]\n",
    "        yaw = camInfo[8]\n",
    "        pitch=camInfo[9]\n",
    "        targetPos = [0.95*curTargetPos[0]+0.05*humanPos[0],0.95*curTargetPos[1]+0.05*humanPos[1],curTargetPos[2]]\n",
    "        \n",
    "        p.resetDebugVisualizerCamera(distance,270 ,pitch,targetPos)\n",
    "\n",
    "    def close(self):\n",
    "        p.disconnect(self.client)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CycleBalancingEnv()\n",
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-product",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space.sample().shape)\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 1\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        action = [0]\n",
    "        state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "        time.sleep(1/24.)\n",
    "        print(state)\n",
    "        clear_output(wait=True)\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape # Shape of our observation space\n",
    "nb_actions = env.action_space.shape[0] # shape of our action space\n",
    "states, nb_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "del actor, critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our actor model for the DDPG algorithm\n",
    "\n",
    "actor = Sequential()\n",
    "actor.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "# actor.add(LSTM(32, input_shape=(8,) + env.observation_space.shape))\n",
    "# actor.add(Flatten())\n",
    "actor.add(Dense(32, kernel_initializer='he_uniform'))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(32, kernel_initializer='he_uniform'))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(32, kernel_initializer='he_uniform'))\n",
    "actor.add(Activation('relu'))\n",
    "# actor.add(Reshape((1, -1)))\n",
    "# actor.add(LSTM(32))\n",
    "actor.add(Dense(nb_actions))\n",
    "actor.add(Activation('tanh'))\n",
    "print(actor.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our critic network for the DDPG algorithm\n",
    "\n",
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "observation_input = tf.keras.Input(shape=(1,) + env.observation_space.shape, name='observation_input')\n",
    "flattened_observation = Flatten()(observation_input)\n",
    "x = Concatenate()([action_input, flattened_observation])\n",
    "x = Dense(32, kernel_initializer='he_uniform')(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(32, kernel_initializer='he_uniform')(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(32, kernel_initializer='he_uniform')(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(1)(x)\n",
    "x = Activation('linear')(x)\n",
    "critic = tf.keras.Model(inputs=[action_input, observation_input], outputs=x)\n",
    "print(critic.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent, SARSAAgent, DDPGAgent\n",
    "#from rl.agents.sarsa import SARSAAgent\n",
    "from rl.policy import BoltzmannQPolicy, BoltzmannGumbelQPolicy, SoftmaxPolicy,  EpsGreedyQPolicy, GreedyQPolicy, BoltzmannGumbelQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "from rl.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_reward = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our DDPG agent\n",
    "\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta= 0.1, mu=0, sigma=.2)\n",
    "agent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=action_input,\n",
    "                  memory=memory, nb_steps_warmup_critic=20, nb_steps_warmup_actor=20,\n",
    "                  random_process=random_process, gamma=0.99, target_model_update=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.compile([Adam(lr=.00001, clipnorm=1.0), Adam(lr=.001, clipnorm=1.0)], metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = agent.fit(env, nb_steps=10000, visualize=True, verbose=2, nb_max_episode_steps=1000)\n",
    "episode_reward += history.history['episode_reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward = []\n",
    "sum_reward = 0\n",
    "span = 100\n",
    "for i in range(len(episode_reward)):\n",
    "    if i>=span: sum_reward -= episode_reward[i-span]\n",
    "    sum_reward += episode_reward[i]\n",
    "    if i>=span: avg_reward.append(sum_reward/span)\n",
    "plt.plot(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward = []\n",
    "sum_reward = 0\n",
    "span = 50\n",
    "for i in range(len(episode_reward)):\n",
    "    if i>=span: sum_reward -= episode_reward[i-span]\n",
    "    sum_reward += episode_reward[i]\n",
    "    if i>=span: avg_reward.append(sum_reward/span)\n",
    "plt.plot(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward = []\n",
    "sum_reward = 0\n",
    "span = 10\n",
    "for i in range(len(episode_reward)):\n",
    "    if i>=span: sum_reward -= episode_reward[i-span]\n",
    "    sum_reward += episode_reward[i]\n",
    "    if i>=span: avg_reward.append(sum_reward/span)\n",
    "plt.plot(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.save_weights('ddpg_{}_weights.h5f'.format('32_3_rays_final'), overwrite=True)\n",
    "# actor.save_weights('actor_32_3_rays_final.h5', overwrite=True) \n",
    "# critic.save_weights('critic_32_3_rays_final.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-drinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.make_obstacles()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time.sleep(10.)\n",
    "_ = agent.test(env, nb_episodes=10, visualize=True) #, nb_max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor.save_weights('actor_32_3_rays_final.h5', overwrite=True) \n",
    "# critic.save_weights('critic_32_3_rays_final.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.load_weights('actor_32_3_rays_final.h5')\n",
    "critic.load_weights('critic_32_3_rays_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_weights('ddpg_{}_weights.h5f'.format('32_3_rays_final'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
