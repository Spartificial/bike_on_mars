{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "subsequent-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "from decimal import Decimal\n",
    "import os\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "undefined-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bicycle and its environment\n",
    "\n",
    "class CycleBalancingEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}  \n",
    "  \n",
    "    def __init__(self):\n",
    "        # Out cycle has only 2 action spaces i.e. Torque of the wheels and the position of the handlebar\n",
    "        self.action_space = gym.spaces.box.Box(\n",
    "            low=-1 * np.ones(1, dtype=np.float32),\n",
    "            high=1 * np.ones(1, dtype=np.float32))\n",
    "        \n",
    "        # Obervation space\n",
    "        self.observation_space = gym.spaces.box.Box(\n",
    "            low=-1 * np.ones(24, dtype=np.float32),\n",
    "            high=1 * np.ones(24, dtype=np.float32))\n",
    "        \n",
    "        self.np_random, _ = gym.utils.seeding.np_random()\n",
    "\n",
    "        if not p.isConnected():\n",
    "            self.client = p.connect(p.GUI) # Physics + Visual\n",
    "            #self.client = p.connect(p.DIRECT) # Only Physics, no visualization. For faster training\n",
    "        else:\n",
    "            self.client = 1\n",
    "        \n",
    "        self.n_target = 200 #Number of obstacles\n",
    "        self.min_target_dist = 5 #Minimum distance of obstacles from bike\n",
    "        self.target_span = 100 #Maximum distance of obstacles from bike\n",
    "        self.sphere_dist = 1.5\n",
    "        #self.pole = []\n",
    "        \n",
    "        p.resetSimulation(self.client)\n",
    "        p.setRealTimeSimulation(0)\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        \n",
    "        self.plane=p.loadURDF(\"plane.urdf\",[0,0,0], useFixedBase=True) #Loading the Plane Surface\n",
    "        self.bike = 0 #Index of the Bike\n",
    "        self.angle_span = 20 #The Angle between two consecutive rays\n",
    "        self.n_episodes = 0 #Count of the number of episodes\n",
    "        self.rays_distance = 30 #The max length of the rays\n",
    "        self.z_balance = -0.25 #Offset between the Bike's center of gravity and the height from which rays are passed\n",
    "        \n",
    "        self.make_obstacles() #create the obstacles\n",
    "        self.reset() #Reseting the environment\n",
    "        \n",
    "        \n",
    "    #Helper (Debugger) function to show the distance traveled by rays in all direction\n",
    "    def show_img(self):\n",
    "        \n",
    "        self.img = np.zeros((800,800,3), dtype='float32')\n",
    "        shift = 400\n",
    "        multiply = 400\n",
    "        ls = p.getBasePositionAndOrientation(self.bike)\n",
    "        bike_x = ls[0][0]\n",
    "        bike_y = ls[0][1]\n",
    "        handlebar_rotation = p.getEulerFromQuaternion( p.getLinkState(self.bike, 0)[1] )[2]\n",
    "        mini = 1000\n",
    "        for deg in range(1, 361, 1):\n",
    "            mini = min(mini, self.dist[deg-1])\n",
    "            if deg%self.angle_span==0:\n",
    "                rad = Decimal( Decimal(deg * np.pi/180 + handlebar_rotation)%Decimal(2*np.pi) + Decimal(2*np.pi))%Decimal(2*np.pi)\n",
    "                rad = float(rad)\n",
    "                start = (int(shift + bike_x + self.sphere_dist*np.cos(rad)), int(shift + bike_y + self.sphere_dist*np.sin(rad)))\n",
    "                end = (int(shift + bike_x + mini*multiply*np.cos(rad)), int(shift + bike_y + mini*multiply*np.sin(rad)))\n",
    "                cv2.ellipse(self.img, start, (int(mini*multiply),int(mini*multiply)), 0, (rad*180/np.pi)-self.angle_span, (rad*180/np.pi), (0,0,255), -1)\n",
    "                mini = 1000\n",
    "            \n",
    "        cv2.imshow('img', cv2.rotate(cv2.transpose(self.img), cv2.ROTATE_180))\n",
    "        #cv2.imshow('img', self.img)\n",
    "        cv2.waitKey(1)\n",
    "        #cv2.destroyAllWindows()\n",
    "        \n",
    "        \n",
    "    def apply_action(self, action):\n",
    "        p.setJointMotorControl2(self.bike, 0, p.POSITION_CONTROL, targetPosition=action[0], maxVelocity=5) # Apply Position control to Handlebar\n",
    "    \n",
    "    \n",
    "    def apply_torque_wheels(self):\n",
    "        p.setJointMotorControl2(self.bike,1,p.TORQUE_CONTROL , force=(2.5+0)*10000) # Apply Toruqe to Back Wheel\n",
    "        p.setJointMotorControl2(self.bike,2,p.TORQUE_CONTROL , force=(2.5+0)*10000) # Apply Toruqe to Front Wheel\n",
    "        \n",
    "        \n",
    "    def gyroscope_torque(self):\n",
    "        ls = p.getBasePositionAndOrientation(self.bike) # ls[0]=Postion of cycle, ls[1] = Orientation of cycle\n",
    "        val = p.getEulerFromQuaternion(ls[1])[0] - 1.57 # Calculating inclination of cycle from vertical\n",
    "        p.applyExternalTorque(self.bike, -1, [-1000000*val, 0, 0], flags=p.WORLD_FRAME)\n",
    "\n",
    "        \n",
    "    def add_pos_orien(self):\n",
    "        ls = p.getBasePositionAndOrientation(self.bike) #Calculating the postion and orientation of the Bike\n",
    "        # ls[0]=Postion of cycle, ls[1] = Orientation of cycle\n",
    "        self.obs += ls[0] #Adding postion\n",
    "        self.obs += p.getEulerFromQuaternion(ls[1]) #Adding orientation\n",
    "        \n",
    "    \n",
    "    def add_dist_by_rays(self):\n",
    "        ls = p.getBasePositionAndOrientation(self.bike) #Calculating the postion and orientation of the Bike\n",
    "            \n",
    "        z = ls[0][2] + self.z_balance #Height of the bike above the ground (Used by rays)\n",
    "            \n",
    "        self.bike_x = ls[0][0] #X-position of the Bike\n",
    "        self.bike_y = ls[0][1] #Y-position of the Bike\n",
    "        \n",
    "        #Calculating the positon and length of the rays (i.e. start and end points of the rays)\n",
    "        bike_x = ls[0][0]\n",
    "        bike_y = ls[0][1]\n",
    "        reward_2 = 0\n",
    "        ray_from = []\n",
    "        ray_to = []\n",
    "        handlebar_rotation = p.getEulerFromQuaternion( p.getLinkState(self.bike, 0)[1] )[2]\n",
    "        for deg in range(1, 361, 1):\n",
    "            rad = Decimal( Decimal(deg * np.pi/180 + handlebar_rotation)%Decimal(2*np.pi) + Decimal(2*np.pi))%Decimal(2*np.pi)\n",
    "            rad = float(rad)\n",
    "            for i in np.arange(0, 3, 1):\n",
    "                for j in np.arange(-4, 1, 0.5):\n",
    "                    ray_from.append((bike_x + self.sphere_dist*np.cos(rad), bike_y + self.sphere_dist*np.sin(rad), z+i))\n",
    "                    ray_to.append((bike_x + self.rays_distance*np.cos(rad), bike_y + self.rays_distance*np.sin(rad), z+j))\n",
    "        \n",
    "        #Adding the observation of the rays (i.e whether the rays collided with any object or not)\n",
    "        rays = p.rayTestBatch(ray_from, ray_to)\n",
    "        mini = 1000\n",
    "        self.dist = []\n",
    "        mini = 1000\n",
    "        cnt = 0\n",
    "        for deg in range(1, 361, 1):\n",
    "            dist = 1\n",
    "            for i in np.arange(0, 3, 1):\n",
    "                for j in np.arange(-4, 1, 0.5):\n",
    "                    tmp = rays[cnt]\n",
    "                    cnt += 1\n",
    "                    if tmp[0]!=self.plane: dist = min(dist, tmp[2])\n",
    "            if dist<mini:\n",
    "                mini = dist\n",
    "            self.dist.append(dist)\n",
    "            if deg%self.angle_span==0:\n",
    "                self.obs.append(mini)\n",
    "                mini = 1000\n",
    "\n",
    "    \n",
    "    def get_collision(self):\n",
    "        ls = p.getContactPoints(self.bike)\n",
    "        reward = 0\n",
    "        done = False\n",
    "        for i in range(len(ls)):\n",
    "            if ls[i][2]!=0:\n",
    "                reward = -100\n",
    "                done = True\n",
    "        return reward, done\n",
    "    \n",
    "    def prevent_rotating_in_a_cycle(self):\n",
    "        ls = p.getBasePositionAndOrientation(self.bike) #Calculating the postion and orientation of the Bike\n",
    "        value = p.getEulerFromQuaternion(p.getLinkState(self.bike, 0)[1])[2] - p.getEulerFromQuaternion(ls[1])[2]\n",
    "        if value<-1: value += 2*np.pi\n",
    "        if value>1: value -= 2*np.pi\n",
    "        #print(value)\n",
    "        if value < -0.5:\n",
    "            self.left += 0.1\n",
    "            self.right = 0\n",
    "        elif value > 0.5:\n",
    "            self.left = 0\n",
    "            self.right += 0.1\n",
    "        else:\n",
    "            self.left = 0\n",
    "            self.right = 0\n",
    "            \n",
    "        self.neg_reward = 0\n",
    "        if self.left>10 or self.right>10:\n",
    "            print(\"Slow!!!\")\n",
    "            self.neg_reward = -100\n",
    "            self.done = True\n",
    "            \n",
    "    def target_dist_achieved(self):\n",
    "        reward_1 = 0\n",
    "        dist_2 = np.sqrt((self.bike_x)**2 + abs(self.bike_y)**2)\n",
    "        if dist_2 > self.target_dist:\n",
    "            reward_1 = 100 + self.target_reward\n",
    "            self.target_dist += 10\n",
    "            self.target_reward = min(500, self.target_reward*2)\n",
    "            \n",
    "        self.completed = 0\n",
    "        if dist_2 > self.target_span:\n",
    "            self.completed = 1\n",
    "            self.done = True\n",
    "            print(\"DONE!\")\n",
    "            self.make_obstacles()\n",
    "        return reward_1\n",
    "    \n",
    "    def get_reward(self, reward_1, reward_2):\n",
    "        dist_2 = np.sqrt((self.bike_x)**2 + abs(self.bike_y)**2)\n",
    "        if self.time%10==0 and dist_2>self.distance: \n",
    "            self.distance = dist_2\n",
    "        return reward_1 + reward_2 + max(-10, (dist_2-self.distance)) + self.time/1000 + self.neg_reward - self.left - self.right\n",
    "    \n",
    "    def get_obs(self):\n",
    "        #For storing all the observations which will be used by the agent to decide the next action\n",
    "        self.obs = [] \n",
    "        \n",
    "        #Add bike's position, orientation to the observation space\n",
    "        self.add_pos_orien() \n",
    "        \n",
    "        #Add info from rays to the observation space (i.e whether rays collided with ant object or not)\n",
    "        self.add_dist_by_rays()\n",
    "        \n",
    "        self.obs = np.array(self.obs, dtype=np.float32)\n",
    "        self.obs[0] /= self.target_span\n",
    "        self.obs[1] /= self.target_span\n",
    "        \n",
    "                \n",
    "    #Step function\n",
    "    def step(self, action):\n",
    "        \n",
    "        self.apply_action(action)\n",
    "        \n",
    "        for i in range(3):\n",
    "            self.apply_torque_wheels() #Apply Torque to the wheels (i.e increase velocity)\n",
    "            self.gyroscope_torque()#Balancing by applying Torque (In real world, this will be done by gyroscope) \n",
    "            p.stepSimulation()\n",
    "            \n",
    "        self.get_obs() #Get the observation\n",
    "        \n",
    "        #Terminate the episode if the Bike collided with any obstacle\n",
    "        reward_2, self.done = self.get_collision() \n",
    "    \n",
    "        # Adding 1 to the time for which the current episode has been running\n",
    "        self.time += 1 \n",
    "                \n",
    "        #Terminate the episode if the Bike keeps rotating in a circle\n",
    "        self.prevent_rotating_in_a_cycle() \n",
    "            \n",
    "        # Terminating the episode if the cycle is more than \"target_span\" distance away from the origin\n",
    "        reward_1 = self.target_dist_achieved()\n",
    "        \n",
    "        # Calculating the total reward\n",
    "        reward = self.get_reward(reward_1, reward_2)\n",
    "\n",
    "        return self.obs, reward/100, self.done, dict()\n",
    "\n",
    "\n",
    "    def load_bike(self):\n",
    "        #Remove the Bike if already loaded\n",
    "        if self.bike!=0:\n",
    "            p.removeBody(self.bike)\n",
    "            \n",
    "        # Loading the Bike\n",
    "        self.bike_x = 0 # random.randint(-5, 5) # X position of the Bike\n",
    "        self.bike_y = 0 # random.randint(-5, 5) # Y position of the Bike\n",
    "        #path = os.getcwd()\n",
    "        self.bike=p.loadURDF('bike_2.urdf.xml',[self.bike_x, self.bike_y,0], p.getQuaternionFromEuler([0,0, random.random()*2*np.pi]),  useFixedBase=False)\n",
    "        \n",
    "    def add_dynamics(self):\n",
    "        # Adding friction and other dynamics\n",
    "        p.changeDynamics(self.plane, -1, lateralFriction=5, angularDamping=1)\n",
    "        p.changeDynamics(self.bike, 1, mass=100)\n",
    "        p.changeDynamics(self.bike, -1, lateralFriction=5, angularDamping=1)\n",
    "        \n",
    "        p.setGravity(0, 0, -250) # Setting the gravity\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        self.n_episodes += 1 #Increase the number of episodes by 1\n",
    "        \n",
    "        #Change obstacles's position after every 20 episodes for robust training\n",
    "        if self.n_episodes==20:\n",
    "            self.make_obstacles()\n",
    "            self.n_episodes = 0\n",
    "        \n",
    "        self.load_bike()\n",
    "        \n",
    "        for i in range(10):\n",
    "            p.stepSimulation()\n",
    "        \n",
    "        self.add_dynamics()\n",
    "        \n",
    "        self.done=False\n",
    "        self.time = 0\n",
    "        self.distance = np.sqrt((self.bike_x)**2 + abs(self.bike_y)**2)\n",
    "        self.neg_reward = 0\n",
    "        \n",
    "        self.get_obs() #Get the observation\n",
    "        \n",
    "        #Initialize variables\n",
    "        self.cnt = 0\n",
    "        self.left = 0\n",
    "        self.right = 0\n",
    "        self.target_dist = 10\n",
    "        self.target_reward = 32\n",
    "        self.completed = 0\n",
    "        \n",
    "        return self.obs\n",
    "\n",
    "    #Function to create the obstacles\n",
    "    def make_obstacles(self):\n",
    "        \n",
    "        p.resetSimulation(self.client)\n",
    "        p.setRealTimeSimulation(0)\n",
    "        #p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        \n",
    "        #Load the obstacles\n",
    "        mul = 100 #Factor to increase or decrease the size of the obstacles\n",
    "        height = 2\n",
    "        visualShift = [0, 0, 0]\n",
    "        shift = [0, 0, 0]\n",
    "        meshScale=[0.1*mul, 0.1*mul, 0.1*mul*height]\n",
    "        #path = 'C:/Users/User/Documents/GitHub/bullet3/examples/pybullet/gym/pybullet_data/'\n",
    "        groundColId = p.createCollisionShape(shapeType=p.GEOM_MESH, \n",
    "                                                  fileName=\"terrain.obj\", \n",
    "                                                  collisionFramePosition=shift,\n",
    "                                                  meshScale=meshScale,\n",
    "                                                  flags=p.GEOM_FORCE_CONCAVE_TRIMESH)\n",
    "        groundVisID = p.createVisualShape(shapeType=p.GEOM_MESH, \n",
    "                                            fileName=\"terrain.obj\", \n",
    "                                            rgbaColor=[0.7,0.3,0.1,1],\n",
    "                                            specularColor=[0.4,.4,0],\n",
    "                                            visualFramePosition=visualShift,\n",
    "                                            meshScale=meshScale)\n",
    "        self.plane = p.createMultiBody(baseMass=0,\n",
    "                                              baseInertialFramePosition=[0,0,0],\n",
    "                                              baseCollisionShapeIndex=groundColId, \n",
    "                                              baseVisualShapeIndex=groundVisID, \n",
    "                                              basePosition=[0,0,0], \n",
    "                                              useMaximalCoordinates=True)\n",
    "        \n",
    "        self.bike = 0\n",
    "        self.bike_x = 0\n",
    "        self.bike_y = 0\n",
    "        self.pole = []\n",
    "        # Load the Target\n",
    "        for i in range(self.n_target):\n",
    "            target_x = self.bike_x\n",
    "            target_y = self.bike_y\n",
    "            while (np.sqrt( (self.bike_x - target_x)**2 + (self.bike_y - target_y)**2 )) < self.min_target_dist:\n",
    "                target_x = random.randint(int(self.bike_x) - self.target_span, int(self.bike_x) + self.target_span)\n",
    "                target_y = random.randint(int(self.bike_y) - self.target_span, int(self.bike_y) + self.target_span)\n",
    "            self.pole.append( p.loadURDF(\"cube.urdf\", [target_x, target_y, 4], [0,0,0,1], useFixedBase=True, globalScaling=1.0) )\n",
    "            p.changeDynamics(self.pole[i], -1, mass=1000)\n",
    "            \n",
    "    \n",
    "    #Render the output Visual\n",
    "    def render(self, mode='human'): \n",
    "        distance=5\n",
    "        yaw = 0\n",
    "        humanPos, humanOrn = p.getBasePositionAndOrientation(self.bike)\n",
    "        humanBaseVel = p.getBaseVelocity(self.bike)\n",
    "        #print(\"frame\",frame, \"humanPos=\",humanPos, \"humanVel=\",humanBaseVel)\n",
    "        camInfo = p.getDebugVisualizerCamera()\n",
    "        curTargetPos = camInfo[11]\n",
    "        distance=camInfo[10]\n",
    "        yaw = camInfo[8]\n",
    "        pitch=camInfo[9]\n",
    "        targetPos = [0.95*curTargetPos[0]+0.05*humanPos[0],0.95*curTargetPos[1]+0.05*humanPos[1],curTargetPos[2]]\n",
    "        \n",
    "        p.resetDebugVisualizerCamera(distance,270 ,pitch,targetPos)\n",
    "\n",
    "    def close(self):\n",
    "        p.disconnect(self.client)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "guided-daughter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the environment\n",
    "env = CycleBalancingEnv()\n",
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "valid-mileage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space =  (24,)\n",
      "Sample Action =  [-0.83013225]\n"
     ]
    }
   ],
   "source": [
    "print('Observation space = ', env.observation_space.sample().shape)\n",
    "print('Sample Action = ', env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "peripheral-turkey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:43.67986993170396\n",
      "DONE!\n",
      "Episode:2 Score:50.70057083005221\n",
      "Episode:3 Score:23.544866203996875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-649191498393>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mn_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mscore\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m240.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-99d50cffa7a8>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstepSimulation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Get the observation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;31m#Terminate the episode if the Bike collided with any obstacle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-99d50cffa7a8>\u001b[0m in \u001b[0;36mget_obs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;31m#Add info from rays to the observation space (i.e whether rays collided with ant object or not)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_dist_by_rays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-99d50cffa7a8>\u001b[0m in \u001b[0;36madd_dist_by_rays\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m                     \u001b[0mray_from\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbike_x\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msphere_dist\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbike_y\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msphere_dist\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                     \u001b[0mray_to\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbike_x\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrays_distance\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbike_y\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrays_distance\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        action = [0.]\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "        time.sleep(1/240.)\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape # Shape of our observation space\n",
    "nb_actions = env.action_space.shape[0] # shape of our action space\n",
    "states, nb_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-pearl",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del actor, critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our actor model for the DDPG algorithm\n",
    "\n",
    "actor = Sequential()\n",
    "actor.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "actor.add(Dense(32, kernel_initializer='he_uniform'))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(32, kernel_initializer='he_uniform'))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(32, kernel_initializer='he_uniform'))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(nb_actions))\n",
    "actor.add(Activation('tanh'))\n",
    "print(actor.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our critic network for the DDPG algorithm\n",
    "\n",
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "observation_input = tf.keras.Input(shape=(1,) + env.observation_space.shape, name='observation_input')\n",
    "flattened_observation = Flatten()(observation_input)\n",
    "x = Concatenate()([action_input, flattened_observation])\n",
    "x = Dense(32, kernel_initializer='he_uniform')(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(32, kernel_initializer='he_uniform')(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(32, kernel_initializer='he_uniform')(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(1)(x)\n",
    "x = Activation('linear')(x)\n",
    "critic = tf.keras.Model(inputs=[action_input, observation_input], outputs=x)\n",
    "print(critic.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from rl.agents import DQNAgent, SARSAAgent, DDPGAgent\n",
    "#from rl.agents.sarsa import SARSAAgent\n",
    "from rl.policy import BoltzmannQPolicy, BoltzmannGumbelQPolicy, SoftmaxPolicy,  EpsGreedyQPolicy, GreedyQPolicy, BoltzmannGumbelQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "from rl.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_reward = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "\n",
    "from rl.core import Agent\n",
    "from rl.policy import Policy\n",
    "from rl.util import *\n",
    "\n",
    "\n",
    "def mean_q(y_true, y_pred):\n",
    "    return K.mean(K.max(y_pred, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "\n",
    "from rl.core import Agent\n",
    "from rl.policy import Policy\n",
    "from rl.util import *\n",
    "\n",
    "\n",
    "def mean_q(y_true, y_pred):\n",
    "    return K.mean(K.max(y_pred, axis=-1))\n",
    "\n",
    "class DDPGAgent(Agent):\n",
    "    \"\"\"\n",
    "    # Arguments\n",
    "        nb_actions: Number of actions\n",
    "        actor, critic: Keras models\n",
    "        critic_action_input: input layer in critic model that corresponds to actor output\n",
    "        enable_twin_delay: enable Twin Delayed DDPG\n",
    "        random_process: noise to add to actor during training when agent performs forward pass\n",
    "        policy_delay: how many steps to delay policy updates by wrt critic updates\n",
    "        noise_sigma: noise to add to actor during training when agent performs backward pass, for critic policy update (TD3)\n",
    "        noise_clip: value to clip above noise by (TD3)\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_actions, actor, critic, critic_action_input, memory,\n",
    "                 gamma=.99, batch_size=32, nb_steps_warmup_critic=1000, nb_steps_warmup_actor=1000,\n",
    "                 train_interval=1, memory_interval=1, delta_range=None, delta_clip=np.inf,\n",
    "                 random_process=None, custom_model_objects={}, target_model_update=.001,\n",
    "                 policy=None, enable_twin_delay=False, policy_delay=None, noise_clip=0.5,\n",
    "                 noise_sigma=0.2, **kwargs):\n",
    "\n",
    "        super(DDPGAgent, self).__init__(**kwargs)\n",
    "\n",
    "        # Soft vs hard target model updates.\n",
    "        if target_model_update < 0:\n",
    "            raise ValueError('`target_model_update` must be >= 0.')\n",
    "        elif target_model_update >= 1:\n",
    "            # Hard update every `target_model_update` steps.\n",
    "            target_model_update = int(target_model_update)\n",
    "        else:\n",
    "            # Soft update with `(1 - target_model_update) * old + target_model_update * new`.\n",
    "            target_model_update = float(target_model_update)\n",
    "\n",
    "        if delta_range is not None:\n",
    "            warnings.warn('`delta_range` is deprecated. Please use `delta_clip` instead, which takes a single scalar. For now we\\'re falling back to `delta_range[1] = {}`'.format(delta_range[1]))\n",
    "            delta_clip = delta_range[1]\n",
    "\n",
    "        # Parameters.\n",
    "        self.enable_twin_delay = enable_twin_delay\n",
    "        self.nb_actions = nb_actions\n",
    "        self.nb_steps_warmup_actor = nb_steps_warmup_actor\n",
    "        self.nb_steps_warmup_critic = nb_steps_warmup_critic\n",
    "        self.random_process = random_process\n",
    "        self.gamma = gamma\n",
    "        self.target_model_update = target_model_update\n",
    "        self.batch_size = batch_size\n",
    "        self.train_interval = train_interval\n",
    "        self.memory_interval = memory_interval\n",
    "        self.custom_model_objects = custom_model_objects\n",
    "        self.policy_delay = policy_delay\n",
    "        if policy_delay is None:\n",
    "            if self.enable_twin_delay:\n",
    "                self.policy_delay = 2\n",
    "            else:\n",
    "                self.policy_delay = 1\n",
    "        if policy is None:\n",
    "            self.policy = DDPGPolicy()\n",
    "        self.noise_clip = noise_clip\n",
    "        self.noise_sigma = noise_sigma\n",
    "        self.delta_clip = delta_clip\n",
    "\n",
    "        # Related objects.\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.critic_action_input = critic_action_input\n",
    "        self.critic_action_input_idx = self.critic.input.index(critic_action_input)\n",
    "        self.memory = memory\n",
    "\n",
    "        # State.\n",
    "        self.compiled = False\n",
    "        self.reset_states()\n",
    "\n",
    "    @property\n",
    "    def uses_learning_phase(self):\n",
    "        return self.actor.uses_learning_phase or self.critic.uses_learning_phase\n",
    "\n",
    "    def compile(self, optimizer, metrics=[]):\n",
    "        metrics += [mean_q]\n",
    "\n",
    "        if type(optimizer) in (list, tuple):\n",
    "            if len(optimizer) != 2:\n",
    "                raise ValueError('More than two optimizers provided. Please only provide a maximum of two optimizers, the first one for the actor and the second one for the critic.')\n",
    "            actor_optimizer, critic_optimizer = optimizer\n",
    "        else:\n",
    "            actor_optimizer = optimizer\n",
    "            critic_optimizer = clone_optimizer(optimizer)\n",
    "        if type(actor_optimizer) is str:\n",
    "            actor_optimizer = optimizers.get(actor_optimizer)\n",
    "        if type(critic_optimizer) is str:\n",
    "            critic_optimizer = optimizers.get(critic_optimizer)\n",
    "        assert actor_optimizer != critic_optimizer\n",
    "\n",
    "        if len(metrics) == 2 and hasattr(metrics[0], '__len__') and hasattr(metrics[1], '__len__'):\n",
    "            actor_metrics, critic_metrics = metrics\n",
    "        else:\n",
    "            actor_metrics = critic_metrics = metrics\n",
    "\n",
    "        def clipped_error(y_true, y_pred):\n",
    "            return K.mean(huber_loss(y_true, y_pred, self.delta_clip), axis=-1)\n",
    "\n",
    "        # Compile target networks. We only use them in feed-forward mode, hence we can pass any\n",
    "        # optimizer and loss since we never use it anyway.\n",
    "        self.target_actor = clone_model(self.actor, self.custom_model_objects)\n",
    "        self.target_actor.compile(optimizer='sgd', loss='mse')\n",
    "        self.target_critic = clone_model(self.critic, self.custom_model_objects)\n",
    "        self.target_critic.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "        # We also compile the actor. We never optimize the actor using Keras but instead compute\n",
    "        # the policy gradient ourselves. However, we need the actor in feed-forward mode, hence\n",
    "        # we also compile it with any optimzer and\n",
    "        self.actor.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "        # Compile the critic.\n",
    "        if self.target_model_update < 1.:\n",
    "            # We use the `AdditionalUpdatesOptimizer` to efficiently soft-update the target model.\n",
    "            critic_updates = get_soft_target_model_updates(self.target_critic, self.critic, self.target_model_update)\n",
    "            critic_optimizer = AdditionalUpdatesOptimizer(critic_optimizer, critic_updates)\n",
    "        self.critic.compile(optimizer=critic_optimizer, loss=clipped_error, metrics=critic_metrics)\n",
    "\n",
    "        # Set up second critic network for TD3\n",
    "        if self.enable_twin_delay:\n",
    "            self.critic2 = clone_model(self.critic, self.custom_model_objects)\n",
    "            self.critic2.compile(optimizer=critic_optimizer, loss=clipped_error, metrics=critic_metrics)\n",
    "            self.target_critic2 = clone_model(self.critic2, self.custom_model_objects)\n",
    "            self.target_critic2.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "        # Combine actor and critic so that we can get the policy gradient.\n",
    "        # Assuming critic's state inputs are the same as actor's.\n",
    "        combined_inputs = []\n",
    "        state_inputs = []\n",
    "        for i in self.critic.inputs:\n",
    "            if i == self.critic_action_input:\n",
    "                combined_inputs.append([])\n",
    "            else:\n",
    "                combined_inputs.append(i)\n",
    "                state_inputs.append(i)\n",
    "        combined_inputs[self.critic_action_input_idx] = self.actor(state_inputs)\n",
    "\n",
    "        combined_output = self.critic(combined_inputs)\n",
    "        updates = actor_optimizer.get_updates(\n",
    "            params=self.actor.trainable_weights, loss=-K.mean(combined_output))\n",
    "        if self.target_model_update < 1.:\n",
    "            # Include soft target model updates.\n",
    "            updates += get_soft_target_model_updates(self.target_actor, self.actor, self.target_model_update)\n",
    "        updates += self.actor.updates  # include other updates of the actor, e.g. for BN\n",
    "\n",
    "        # Finally, combine it all into a callable function.\n",
    "        if K.backend() == 'tensorflow':\n",
    "            self.actor_train_fn = K.function(state_inputs + [K.learning_phase()],\n",
    "                                             [self.actor(state_inputs)], updates=updates)\n",
    "        else:\n",
    "            if True: #self.uses_learning_phase:\n",
    "                state_inputs += [K.learning_phase()]\n",
    "            self.actor_train_fn = K.function(state_inputs, [self.actor(state_inputs)], updates=updates)\n",
    "        self.actor_optimizer = actor_optimizer\n",
    "\n",
    "        self.compiled = True\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        filename, extension = os.path.splitext(filepath)\n",
    "        actor_filepath = filename + '_actor' + extension\n",
    "        critic_filepath = filename + '_critic' + extension\n",
    "        self.actor.load_weights(actor_filepath)\n",
    "        self.critic.load_weights(critic_filepath)\n",
    "        self.update_target_models_hard()\n",
    "\n",
    "    def save_weights(self, filepath, overwrite=False):\n",
    "        filename, extension = os.path.splitext(filepath)\n",
    "        actor_filepath = filename + '_actor' + extension\n",
    "        critic_filepath = filename + '_critic' + extension\n",
    "        self.actor.save_weights(actor_filepath, overwrite=overwrite)\n",
    "        self.critic.save_weights(critic_filepath, overwrite=overwrite)\n",
    "\n",
    "    def update_target_models_hard(self):\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        if self.enable_twin_delay:\n",
    "            self.target_critic2.set_weights(self.critic2.get_weights())\n",
    "\n",
    "    # TODO: implement pickle\n",
    "\n",
    "    def reset_states(self):\n",
    "        if self.random_process is not None:\n",
    "            self.random_process.reset_states()\n",
    "        self.recent_action = None\n",
    "        self.recent_observation = None\n",
    "        if self.compiled:\n",
    "            self.actor.reset_states()\n",
    "            self.critic.reset_states()\n",
    "            self.target_actor.reset_states()\n",
    "            self.target_critic.reset_states()\n",
    "            if self.enable_twin_delay:\n",
    "                self.critic2.reset_states()\n",
    "                self.target_critic2.reset_states()\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        batch = np.array(batch)\n",
    "        if self.processor is None:\n",
    "            return batch\n",
    "        return self.processor.process_state_batch(batch)\n",
    "\n",
    "    def forward(self, observation):\n",
    "        # Select an action.\n",
    "        state = self.memory.get_recent_state(observation)\n",
    "        batch = self.process_state_batch([state])\n",
    "        action = self.actor.predict_on_batch(batch).flatten()\n",
    "        assert action.shape == (self.nb_actions,)\n",
    "\n",
    "        if self.training:\n",
    "            action = self.policy.select_action(action, self.random_process,\n",
    "                                               noise_clip=None)\n",
    "\n",
    "        # Book-keeping.\n",
    "        self.recent_observation = observation\n",
    "        self.recent_action = action\n",
    "\n",
    "        return action\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return self.actor.layers[:] + self.critic.layers[:]\n",
    "\n",
    "    @property\n",
    "    def metrics_names(self):\n",
    "        names = self.critic.metrics_names[:]\n",
    "        if self.enable_twin_delay:\n",
    "            names += self.critic2.metrics_names[:]\n",
    "        if self.processor is not None:\n",
    "            names += self.processor.metrics_names[:]\n",
    "        return names\n",
    "\n",
    "    def backward(self, reward, terminal=False):\n",
    "        # Store most recent experience in memory.\n",
    "        if self.step % self.memory_interval == 0:\n",
    "            self.memory.append(self.recent_observation, self.recent_action, reward, terminal,\n",
    "                               training=self.training)\n",
    "\n",
    "        metrics = [np.nan for _ in self.metrics_names]\n",
    "        if not self.training:\n",
    "            # We're done here. No need to update the experience memory since we only use the working\n",
    "            # memory to obtain the state over the most recent observations.\n",
    "            return metrics\n",
    "\n",
    "        # Train the network on a single stochastic batch.\n",
    "        can_train_either = self.step > self.nb_steps_warmup_critic or self.step > self.nb_steps_warmup_actor\n",
    "        if can_train_either and self.step % self.train_interval == 0:\n",
    "            experiences = self.memory.sample(self.batch_size)\n",
    "            assert len(experiences) == self.batch_size\n",
    "\n",
    "            # Start by extracting the necessary parameters (we use a vectorized implementation).\n",
    "            state0_batch = []\n",
    "            reward_batch = []\n",
    "            action_batch = []\n",
    "            terminal1_batch = []\n",
    "            state1_batch = []\n",
    "            for e in experiences:\n",
    "                state0_batch.append(e.state0)\n",
    "                state1_batch.append(e.state1)\n",
    "                reward_batch.append(e.reward)\n",
    "                action_batch.append(e.action)\n",
    "                terminal1_batch.append(0. if e.terminal1 else 1.)\n",
    "\n",
    "            # Prepare and validate parameters.\n",
    "            state0_batch = self.process_state_batch(state0_batch)\n",
    "            state1_batch = self.process_state_batch(state1_batch)\n",
    "            terminal1_batch = np.array(terminal1_batch)\n",
    "            reward_batch = np.array(reward_batch)\n",
    "            action_batch = np.array(action_batch)\n",
    "            assert reward_batch.shape == (self.batch_size,)\n",
    "            assert terminal1_batch.shape == reward_batch.shape\n",
    "            assert action_batch.shape == (self.batch_size, self.nb_actions)\n",
    "\n",
    "            # Update critic, if warm up is over.\n",
    "            if self.step > self.nb_steps_warmup_critic:\n",
    "                target_actions = self.target_actor.predict_on_batch(state1_batch)\n",
    "                if self.enable_twin_delay:\n",
    "                    # Add clipped noise to target actions\n",
    "                    target_actions = self.policy.select_action(target_actions, noise_sigma=self.noise_sigma, noise_clip=self.noise_clip)\n",
    "                assert target_actions.shape == (self.batch_size, self.nb_actions)\n",
    "                if len(self.critic.inputs) >= 3:\n",
    "                    state1_batch_with_action = state1_batch[:]\n",
    "                else:\n",
    "                    state1_batch_with_action = [state1_batch]\n",
    "                state1_batch_with_action.insert(self.critic_action_input_idx, target_actions)\n",
    "                if self.enable_twin_delay:\n",
    "                    target_q1_values = self.target_critic.predict_on_batch(state1_batch_with_action).flatten()\n",
    "                    target_q2_values = self.target_critic2.predict_on_batch(state1_batch_with_action).flatten()\n",
    "                    target_q_values = np.minimum(target_q1_values, target_q2_values)\n",
    "                else:\n",
    "                    target_q_values = self.target_critic.predict_on_batch(state1_batch_with_action).flatten()\n",
    "                assert target_q_values.shape == (self.batch_size,)\n",
    "\n",
    "                # Compute r_t + gamma * max_a Q(s_t+1, a) and update the target ys accordingly,\n",
    "                # but only for the affected output units (as given by action_batch).\n",
    "                discounted_reward_batch = self.gamma * target_q_values\n",
    "                discounted_reward_batch *= terminal1_batch\n",
    "                assert discounted_reward_batch.shape == reward_batch.shape\n",
    "                targets = (reward_batch + discounted_reward_batch).reshape(self.batch_size, 1)\n",
    "\n",
    "                # Perform a single batch update on the critic network.\n",
    "                if len(self.critic.inputs) >= 3:\n",
    "                    state0_batch_with_action = state0_batch[:]\n",
    "                else:\n",
    "                    state0_batch_with_action = [state0_batch]\n",
    "                state0_batch_with_action.insert(self.critic_action_input_idx, action_batch)\n",
    "                metrics = self.critic.train_on_batch(state0_batch_with_action, targets)\n",
    "                if not isinstance(metrics, list):\n",
    "                    metrics = [metrics]\n",
    "                if self.enable_twin_delay:\n",
    "                    metrics2 = self.critic2.train_on_batch(state0_batch_with_action, targets)\n",
    "                    if not isinstance(metrics, list):\n",
    "                        metrics2 = [metrics2]\n",
    "                    metrics += metrics2\n",
    "                if self.processor is not None:\n",
    "                    metrics += self.processor.metrics\n",
    "\n",
    "            # Update actor, if warm up is over.\n",
    "            if self.step > self.nb_steps_warmup_actor and self.step % self.policy_delay == 0:\n",
    "                # TODO: implement metrics for actor\n",
    "                if len(self.actor.inputs) >= 2:\n",
    "                    inputs = state0_batch[:]\n",
    "                else:\n",
    "                    inputs = [state0_batch]\n",
    "                if True: #self.uses_learning_phase:\n",
    "                    inputs += [self.training]\n",
    "                action_values = self.actor_train_fn(inputs)[0]\n",
    "                assert action_values.shape == (self.batch_size, self.nb_actions)\n",
    "\n",
    "        if self.target_model_update >= 1 and self.step % self.target_model_update == 0:\n",
    "            self.update_target_models_hard()\n",
    "\n",
    "        return metrics\n",
    "\n",
    "class DDPGPolicy(Policy):\n",
    "    \"\"\"\n",
    "    Adds noise sampled from random_process to action.\n",
    "    If random_process is None, draws from N(0, noise_sigma) distribution.\n",
    "    If noise_clip is not None, clips noise to [-noise_clip, noise_clip]\n",
    "    \"\"\"\n",
    "    def select_action(self, action, random_process=None,\n",
    "                      noise_sigma=0.2, noise_clip=None):\n",
    "        if random_process is not None:\n",
    "            noise = random_process.sample()\n",
    "        else:\n",
    "            noise = np.random.normal(0, noise_sigma, action.shape)\n",
    "        if noise_clip is not None:\n",
    "            noise = np.clip(noise,\n",
    "                            -noise_clip * np.ones(noise.shape),\n",
    "                            noise_clip * np.ones(noise.shape))\n",
    "        assert noise.shape == action.shape\n",
    "        action += noise\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our DDPG agent\n",
    "\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta= 0.1, mu=0, sigma=.2)\n",
    "agent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=action_input, batch_size=1024,\n",
    "                  memory=memory, nb_steps_warmup_critic=20, nb_steps_warmup_actor=20,\n",
    "                  random_process=random_process, gamma=0.95, target_model_update=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.compile([Adam(lr=.0001, clipnorm=1.0), Adam(lr=.001, clipnorm=1.0)], metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-underwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = agent.fit(env, nb_steps=20000, visualize=True, verbose=2, nb_max_episode_steps=1000)\n",
    "episode_reward += history.history['episode_reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward = []\n",
    "sum_reward = 0\n",
    "span = 100\n",
    "for i in range(len(episode_reward)):\n",
    "    if i>=span: sum_reward -= episode_reward[i-span]\n",
    "    sum_reward += episode_reward[i]\n",
    "    if i>=span: avg_reward.append(sum_reward/span)\n",
    "plt.plot(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward = []\n",
    "sum_reward = 0\n",
    "span = 50\n",
    "for i in range(len(episode_reward)):\n",
    "    if i>=span: sum_reward -= episode_reward[i-span]\n",
    "    sum_reward += episode_reward[i]\n",
    "    if i>=span: avg_reward.append(sum_reward/span)\n",
    "plt.plot(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward = []\n",
    "sum_reward = 0\n",
    "span = 10\n",
    "for i in range(len(episode_reward)):\n",
    "    if i>=span: sum_reward -= episode_reward[i-span]\n",
    "    sum_reward += episode_reward[i]\n",
    "    if i>=span: avg_reward.append(sum_reward/span)\n",
    "plt.plot(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.save_weights('ddpg_{}_weights.h5f'.format('32_3_rays_final'), overwrite=True)\n",
    "# actor.save_weights('actor_32_3_rays_final.h5', overwrite=True) \n",
    "# critic.save_weights('critic_32_3_rays_final.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.make_obstacles()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time.sleep(10.)\n",
    "_ = agent.test(env, nb_episodes=50, visualize=True, nb_max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.load_weights('actor_32_3_rays_final.h5')\n",
    "critic.load_weights('critic_32_3_rays_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_weights('ddpg_{}_weights.h5f'.format('32_3_rays_final'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-determination",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
